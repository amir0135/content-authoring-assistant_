# Content Authoring Assistant API

## Table of Contents

1. [Overview](#overview)
2. [Setup and Running the Application](#setup-and-running-the-application)
    - [Prerequisites](#prerequisites)
    - [Running Locally](#running-locally)
    - [Running in Docker](#running-in-docker)
3. [Deployment to Google Cloud](#deployment-to-google-cloud)
4. [API Endpoints](#api-endpoints)
    - [Example Request via curl](#example-request-via-curl)
5. [Evaluation of Quality](#evaluation-of-quality)
6. [Optimization Strategies](#optimization-strategies)
7. [Scaling the Model](#scaling-the-model)
8. [Technical Considerations for Production](#technical-considerations-for-production)
9. [Conceptual and Technical Design](#conceptual-and-technical-design)
10. [Strategic Roadmap](#strategic-roadmap)

## Overview

The **Content Authoring Assistant API** is a scalable, cloud-native FastAPI application designed for generating domain-specific content based on user inputs like keywords, domain, audience, tone, and more. It leverages OpenAI's `gpt-3.5-turbo` model for text generation, containerized with Docker, and deployed on Google Cloud for scalability and high availability.

The API is built to handle multiple concurrent requests with low latency and can auto-scale to meet varying traffic demands, ensuring a responsive experience for users who need quick, high-quality content.

## Setup and Running the Application

### Prerequisites

- Python 3.11+
- Docker
- Google Cloud SDK (if deploying to Google Cloud)
- OpenAI API Key

### Running Locally

1. **Clone the repository:**

    ```bash
    git clone https://github.com/amir0135/content-authoring-assistant.git
    cd content-authoring-assistant
    ```

2. **Set up environment variables:**

    Create a `.env` file inside the `config/` directory and add your OpenAI API key:

    ```plaintext
    OPENAI_API_KEY=your_openai_api_key
    ```

3. **Install dependencies:**

    ```bash
    pip install -r config/requirements.txt
    ```

4. **Run the application:**

    ```bash
    uvicorn src.main:app --reload
    ```

5. The API will be accessible at `http://localhost:8000`.

### Running in Docker

1. **Build the Docker image:**

    ```bash
    docker build -t content-authoring-assistant -f infra/Dockerfile .
    ```

2. **Run the Docker container:**

    ```bash
    docker run -p 8000:8000 --env-file config/.env content-authoring-assistant
    ```

3. Access the API at `http://localhost:8000`.

## Deployment to Google Cloud

1. **Build and push the Docker image:**

    ```bash
    gcloud builds submit --tag gcr.io/[PROJECT-ID]/content-authoring-assistant
    ```

2. **Deploy to Google Cloud Run:**

    ```bash
    gcloud run deploy --image gcr.io/[PROJECT-ID]/content-authoring-assistant --platform managed --region europe-west1 --allow-unauthenticated
    ```

## API Endpoints

- `POST /generate-text/`: Generates text based on the input parameters such as prompt, domain, audience, tone, etc.

  Example request:

  ```json
  {
      "prompt": "Write a blog post about AI advancements.",
      "max_tokens": 100,
      "temperature": 0.7,
      "top_p": 0.9,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0,
      "domain": "technology",
      "audience": "general",
      "tone": "informative",
      "keywords": "AI, technology"
  }
  ```

### Example Request via curl

You can also test my deployed API by making a request to the Kubernetes-deployed endpoint:

```bash
curl -X POST "http://35.240.49.235/generate-text/" \
-H "Content-Type: application/json" \
-d '{
  "prompt": "AI and machine learning",
  "domain": "technology",
  "audience": "developers",
  "tone": "formal",
  "keywords": "innovation, automation",
  "max_tokens": 100
}'
```

This will return a generated text response from the model based on the provided parameters.

## Evaluation of Quality

### Approaches

1. **Human Evaluation**: Review the generated text for coherence, relevance, and fluency.
2. **Automated Metrics**: Use metrics like BLEU and ROUGE to evaluate text quality.
3. **Performance Testing**: Measure response time and throughput using `test.js` for load testing.

### Performance Testing

To test the system under load, run the following:

```bash
node src/test.js
```

## Optimization Strategies

1. **Model Distillation**: Consider using smaller models like `distilgpt2` for faster inference.
2. **Batch Requests**: Combine multiple requests into a single API call to improve efficiency.
3. **Caching**: Cache frequently used prompts to reduce redundant API calls to OpenAI.
4. **Quantization**: Convert model weights to lower precision (e.g., 8-bit) for faster computation.
5. **Pruning**	Remove less critical parts of the model to reduce size and speed up inference.

## Scaling the Model

1. **Horizontal Scaling with HPA**: Kubernetes Horizontal Pod Autoscaler (HPA) can be used to automatically scale based on CPU or memory utilization.
2. **Multilingual Models**: Expand support by training or fine-tuning on multilingual datasets.
3. **Custom Fine-Tuning**: Train the model on specific domain data for better relevance.

## Technical Considerations for Production

1. **Security**: Ensure secure API access with API keys, OAuth, and HTTPS.
2. **Latency**: Optimize response times by fine-tuning models and limiting output lengths.
3. **Concurrency**: Google Cloud Run allows for 10 concurrent requests and can autoscale to handle more.
4. **Data Residency**: Ensure GDPR compliance by keeping all data processing within the EU.

## Conceptual and Technical Design

### Key Tools and Technologies:

1. **API Framework**: FastAPI for developing the REST API.
2. **Text Generation**: OpenAIâ€™s GPT-3.5 Turbo model.
3. **Containerization**: Docker for creating reproducible environments.
4. **Cloud Deployment**: Google Cloud Run for scalable serverless deployment.
5. **Monitoring & Scaling**: Kubernetes with HPA to handle scaling and performance monitoring.

### Architecture:

- **API Gateway**: Handles incoming requests from users.
- **Service Layer**: Processes requests and prepares input for the model.
- **Model Layer**: GPT-3.5 Turbo generates the content based on input.
- **Auto-scaling**: Kubernetes HPA for scaling based on traffic demands.

## Strategic Roadmap

1. **Short-Term**: Focus on optimizing response times and adding more input parameters.
2. **Mid-Term**: Expand the model to support multiple languages and fine-tune for domain-specific tasks.
3. **Long-Term**: Implement feedback loops for continuous improvement and integrate automated evaluation methods for generated content.

